<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="backup-restore-ucp-dtr">
<title>Backup and restore UCP and DTR</title>
<body>

<section>
<title>Introduction</title>
<p>The playbooks provided in this solution implement the backup and restore procedures as they
are described in the Docker documentation at <xref
href="https://docs.docker.com/enterprise/backup/" format="html"/></p>
<ul>
<li><codeph>playbooks/backup_swarm.yml</codeph> is used to backup the swarm data</li>
<li><codeph>playbooks/backup_ucp.yml</codeph> is used to backup UCP</li>
<li><codeph>playbooks/backup_dtr_meta.yml</codeph> is used to backup DTR metadata</li>
<li><codeph>playbooks/backup_dtr_images.yml</codeph> is used to backup DTR images</li>
</ul>
  
<note type="note">It is important that you make copies of the backed up data and that you store the copies in a separate physical location. 
You must also recognize that the backed up data contains sensitive information such as private keys and so it is important to restrict
access to the generated files. However, the playbooks do not backup the sensitive information in your <codeph>group_vars/vault</codeph> file so you should make 
sure to keep track of the credentials for the UCP Administrator. </note>  
  
</section>

<section>
<title>Backup variables</title>
<p><xref href="backup-restore-ucp-dtr.dita#backup-restore-ucp-dtr/backup-ucp-table-content"/>
shows the variables related to backing up UCP and DTR. All these variables are defined in
the file <b>group_vars/backup</b>. All the data that is backed up is streamed over an SSH
connection to the backup server. Currently, the playbooks only support the use of the
Ansible box as the backup server.</p>
<table id="backup-ucp-table-content" frame="none">
<title>Backup variables</title>
<!--<colgroup xmlns:ac="ac" xmlns:ri="ri">
<col/>
<col/>
</colgroup>-->

<tgroup cols="3">
<colspec colnum="1" colname="c1" align="left"/>
<colspec colnum="2" colname="c2" align="left"/>
<colspec colnum="3" colname="c3" align="left"/>
<thead>
<row>
<entry>Variable</entry>
<entry>File</entry>
<entry>Description</entry>
</row>
</thead>
<tbody>
<row>
<entry>backup_server</entry>
<entry><b>group_vars/backup</b></entry>
<entry>Currently, the playbooks only support the use of the Ansible box as the backup
server. </entry>
</row>
<row>
<entry>backup_dest</entry>
<entry><b>group_vars/backup</b></entry>
<entry>This variable should point to an existing folder on your ansible box where the
root use has write access. All the backups will be stored in this folder. For
example, <codeph>/root/backup</codeph></entry>
</row>
<row>
<entry>#swarm_offline_backup</entry>
<entry><b>group_vars/backup</b></entry>
<entry>This variable is commented out by default. More information on the use of this variable is provided below. </entry>
</row>    
</tbody>
</tgroup>
</table>
</section>

<section>
<title>Backing up the Swarm</title>
<p> When you backup the swarm, your services and stacks definitions are backed up as well as
networks definitions. However, Docker volumes or their contents will not be backed up. (If
Docker volumes are defined in stacks, they will be re-created when you restore the stacks,
but their content will be lost). You can backup the swarm using the playbook named
<codeph>backup_swarm.yml</codeph> which is located in the <codeph>playbooks</codeph>
folder on your Ansible server. The playbook is invoked as follows: </p>
<codeblock># ansible-playbook -i vm_hosts playbooks/backup_swarm.yml</codeblock>
<p>This playbook creates two archives in the folder specified by the variable
<codeph>backup_dest</codeph> in <codeph>group_vars/backup</codeph>. By default, the file
is named using the following pattern:</p>
    
<codeblock>&lt;backup_dest>/backup_swarm_&lt;vmname>_&lt;timestamp>.tgz
&lt;backup_dest>/backup_swarm_&lt;vmname>_&lt;timestamp>.vars.tgz
</codeblock>
    
<p>where <codeph>&lt;vmname></codeph> is the name of the host (in the inventory) that was used
to take the backup, and <codeph>&lt;timestamp></codeph> is the time at which the backup was
taken. The file with the extension <codeph>.vars.tgz</codeph> contains information 
regarding the system that was backed up.</p>
    
<p>You can override the generated file name by defining the variable <b>backup_name</b> on the
command line when running the playbook. In the example below: </p>
<codeblock># ansible-playbook -i vm_hosts playbooks/backup_swarm.yml -e backup_name=<b>my_swarm_backup</b></codeblock>
<p>the generated files won't have <codeph>&lt;vmname></codeph> or
<codeph>&lt;timestamp></codeph> appended:</p>
    
<codeblock>&lt;backup_dest>/my_swarm_backup.tgz
&lt;backup_dest>/my_swarm_backup.vars.tgz
</codeblock>

    <note type="warning"><b>Online versus offline backups:</b> By default, The playbook performs online backups.
You can take offline backups by setting the variable <codeph>swarm_backup_offline</codeph> to <codeph>"true"</codeph>.
The playbook will then stop the Docker daemon on the machine used to take the backup (a manager/UCP node).
Before it does so, the playbook will verify that enough managers are running in the cluster to
maintain the quroum. If this is not the case, the playbook will exit with an error. For more information, see the Docker
documentation at <xref
href="https://docs.docker.com/engine/swarm/admin_guide/#recover-from-disasterv"
format="html"/></note>
</section>

<section>
<title>Backing up the Universal Control Plane (UCP)</title>
<p>When you backup UCP, you save the following data/metadata: </p>
<table id="backup-ucp-data-meta-table-content" frame="none">
<title>UCP data backed up</title>
<!--<colgroup xmlns:ac="ac" xmlns:ri="ri">
<col/>
<col/>
</colgroup>-->

<tgroup cols="2">
<colspec colnum="1" colname="c1" align="left"/>
<colspec colnum="2" colname="c2" align="left"/>
<thead>
<row>
<entry>Data</entry>
<entry>Description</entry>
</row>
</thead>
<tbody>
<row>
<entry>Configurations</entry>
<entry>The UCP cluster configurations, as shown by <codeph>docker config ls</codeph>,
including Docker EE license and swarm and client CAs </entry>
</row>
<row>
<entry>Access control</entry>
<entry>Permissions for team access to swarm resources, including collections, grants, and
roles</entry>
</row>
<row>
<entry>Certificates and keys</entry>
<entry>The certificates, public keys, and private keys that are used for
authentication and mutual TLS communication</entry>
</row>
<row>
<entry>Metrics data</entry>
<entry>Monitoring data gathered by UCP</entry>
</row>
<row>
<entry>Organizations</entry>
<entry>Your users, teams, and orgs</entry>
</row>
<row>
<entry>Volumes</entry>
<entry>All <xref
href="https://docs.docker.com/datacenter/ucp/2.2/guides/architecture/#volumes-used-by-ucp"
format="html">UCP named volumes</xref>, which include all UCP component certs and
data</entry>
</row>
</tbody>
</tgroup>
</table>
    
<p>To make a backup of UCP, use <codeph>playbook/backup_ucp.yml</codeph> as follows:</p>
<codeblock># ansible-playbook -i vm_host playbooks/backup_ucp.yml</codeblock>
    
<p>This playbook creates two archives in the folder specified by the variable
<codeph>backup_dest</codeph> in <codeph>group_vars/backup</codeph>. By default, the files
are named using the following pattern:</p>
    
<codeblock>&lt;backup_dest>/backup_ucp_&lt;ucpid>_&lt;vmname>_&lt;timestamp>.tgz
&lt;backup_dest>/backup_ucp_&lt;ucpid>_&lt;vmname>_&lt;timestamp>.vars.tgz
</codeblock>
    
<p>where <codeph>&lt;ucpid></codeph> is the ID of the UCP instance,
<codeph>&lt;vmname></codeph> is the name of the host (in the inventory) that was used to
take the backup, and <codeph>&lt;timestamp></codeph> is the time at which the backup was
taken. The file with the extension <codeph>.vars.tgz</codeph> contains information regarding the system which was backed up.</p>
    
<p>You can override the generated file name by defining the variable <b>backup_name</b> on the
command line when running the playbook. In the example below: </p>
<codeblock># ansible-playbook -i vm_hosts playbooks/backup_ucp.yml -e backup_name=<b>my_ucp_backup</b></codeblock>
<p>the generated files won't have <codeph>&lt;vmname></codeph> or
<codeph>&lt;timestamp></codeph> appended:</p>
    
<codeblock>&lt;backup_dest>/my_ucp_backup.tgz
&lt;backup_dest>/my_ucp_backup.vars.tgz
</codeblock>
    
<note type="warning">To create a consistent backup, the backup command <b>temporarily stops the
UCP containers running on the node where the backup is being performed</b>. User resources, such
as services, containers, and stacks are not affected by this operation and will continue
to operate as expected. Any long-lasting <codeph>docker exec</codeph>, <codeph>docker logs</codeph>, <codeph>docker events</codeph>, 
or <codeph>docker attach</codeph> operations on the
affected manager node will be disconnected.</note>
  
<p>For more information on UCP backup, see the Docker documentation at
<xref href="https://docs.docker.com/datacenter/ucp/2.2/guides/admin/backups-and-disaster-recovery/" format="html"/></p>  
  
</section>


<section>
<title>Backing up the Docker Trusted Registry (DTR)</title>
<p>When you backup DTR, you save the following data/metadata: </p>
<table id="backup-dtr-data-meta-table-content" frame="none">
<title>UCP data backed up</title>
<tgroup cols="3">
<colspec colnum="1" colname="c1" align="left"/>
<colspec colnum="2" colname="c2" align="left"/>
<colspec colnum="3" colname="c3" align="left"/>
<thead>
<row>
<entry>Data</entry>
<entry>Backed up?</entry>
<entry>Description</entry>
</row>
</thead>
<tbody>
<row>
<entry>Configurations</entry>
<entry>yes</entry>
<entry>DTR settings</entry>
</row>
<row>
<entry>Repository metadata</entry>
<entry>yes</entry>
<entry>Metadata like image architecture and size</entry>
</row>
<row>
<entry>Access control to repos and images</entry>
<entry>yes</entry>
<entry>Data about who has access to which images</entry>
</row>
<row>
<entry>Notary data</entry>
<entry>yes</entry>
<entry>Signatures and digests for images that are signed</entry>
</row>
<row>
<entry>Scan results</entry>
<entry>yes</entry>
<entry>Information about vulnerabilities in your images</entry>
</row>
<row>
<entry>Certificates and keys</entry>
<entry>yes</entry>
<entry>TLS certificates and keys used by DTR</entry>
</row>
<row>
<entry>Image content</entry>
<entry>no</entry>
<entry>Needs to be backed up separately, depends on DTR configuration</entry>
</row>
<row>
<entry>Users, orgs, teams</entry>
<entry>no</entry>
<entry>Create a UCP backup to backup this data</entry>
</row>
<row>
<entry>Vulnerability database</entry>
<entry>no</entry>
<entry>Can be re-downloaded after a restore</entry>
</row>  
</tbody>
</tgroup>
</table>


<p>To make a backup of DTR metadata, use <codeph>playbook/backup_dtr_metadata.yml</codeph></p>
<codeblock># ansible-playbook -i vm_host playbooks/backup_dtr_metadata.yml</codeblock>
<p>This playbook creates two archives in the folder specified by the variable
<codeph>backup_dest</codeph> in <codeph>group_vars/backup</codeph>. By default, the file
is named using the following pattern:</p>
    
<codeblock>&lt;backup_dest>/backup_dtr_meta_&lt;replica_id>_&lt;vmname>_&lt;timestamp>.tgz
&lt;backup_dest>/backup_dtr_meta_&lt;replica_id>_&lt;vmname>_&lt;timestamp>.vars.tgz
</codeblock>
    
<p>where <codeph>&lt;replica_id></codeph> is the ID of the DTR replica that was backed up,
<codeph>&lt;vmname></codeph> is the name of the host (in the inventory) that was used to
take the backup, and <codeph>&lt;timestamp></codeph> is the time at which the backup was
taken. The file with the extension <codeph>.vars.tgz</codeph> contains information regarding the system that was backed up.</p>
  
<p>You can override the generated file name by defining the variable <b>backup_name</b> on the
command line when running the playbook. In the example below: </p>
<codeblock># ansible-playbook -i vm_hosts playbooks/backup_dtr_metadata.yml -e backup_name=<b>my_dtr_metadata_backup</b></codeblock>
<p>the generated files won't have <codeph>&lt;vmname></codeph> or
<codeph>&lt;timestamp></codeph> appended:</p>
    
<codeblock>&lt;backup_dest>/my_dtr_metadata_backup.tgz
&lt;backup_dest>/my_dtr_metadata_backup.vars.tgz
</codeblock>

<p>For more information on DTR backups, see the Docker documentation at 
<xref href="https://docs.docker.com/datacenter/dtr/2.4/guides/admin/backups-and-disaster-recovery/" format="html"/></p>

</section>


<section>
<title>Backing up DTR data (images)</title>  
  
<p>To make a backup of the images that are inventoried in DTR and stored on the NFS server, use <codeph>playbooks/backup_dtr_images.yml</codeph> </p>  
  
<codeblock># ansible-playbook -i vm_host playbooks/backup_dtr_images.yml</codeblock>  
  
<p>This playbook creates two archives in the folder specified by the variable
<codeph>backup_dest</codeph> in <codeph>group_vars/backup</codeph>. By default, the file
is named using the following pattern:</p>
    
<codeblock>&lt;backup_dest>/backup_dtr_data_&lt;replica_id>_&lt;vmname>_&lt;timestamp>.tgz
&lt;backup_dest>/backup_dtr_data_&lt;replica_id>_&lt;vmname>_&lt;timestamp>.vars.tgz
</codeblock>

<p>where <codeph>&lt;replica_id></codeph> is the ID of the DTR replica that was backed up,
<codeph>&lt;vmname></codeph> is the name of the host (in the inventory) that was used to
take the backup, and <codeph>&lt;timestamp></codeph> is the time at which the backup was
taken.</p>  
  
<p>You can override the generated file name by defining the variable <b>backup_name</b> on the
command line when running the playbook. In the example below: </p>
<codeblock># ansible-playbook -i vm_hosts playbooks/backup_dtr_images.yml -e backup_name=<b>my_dtr_data_backup</b></codeblock>
<p>the generated file won't have <codeph>&lt;vmname></codeph> or
<codeph>&lt;timestamp></codeph> appended:</p>
    
<codeblock>&lt;backup_dest>/my_dtr_data_backup.tgz
&lt;backup_dest>/my_dtr_data_backup.vars.tgz
</codeblock>  
  
<p>For more information on DTR backups, see the Docker documentation at 
<xref href="https://docs.docker.com/datacenter/dtr/2.4/guides/admin/backups-and-disaster-recovery/" format="html"/></p>
  
  
</section>
    
<section>
<title>Backing up other metadata, including passwords</title>   
    
<p>The backup playbooks do not backup the sensitive data in your <codeph>group_vars/vault</codeph> file. 
The information stored in the <codeph>.vars.tgz</codeph> files includes backups of the following files:</p>
    
<ul>
<li><b>vm_hosts</b>, a copy of the <codeph>vm_hosts</codeph> file at the time the backup was taken</li>
<li><b>vars</b>, a copy of your <codeph>group_vars/vars</codeph> file at the time the backup was taken</li>
<li><b>meta.yml</b>, a generated file containing information pertaining to the backup</li>    
</ul>    

<p>The <b>meta.yml</b> file contains the following information:</p>
    
<codeblock>backup_node="&lt;node that took the backup>"
replica_id="&lt;ID of DTR replica if DTR backup>"
backup_source=""
ucp_version="&lt;UCP version if UCP backup>"
dtr_version="&lt;DTR version of DTR backup>"</codeblock>    


    
</section>    


<section>
<title>Backup Utility</title>    
<p>The script <codeph>backup.sh</codeph> can be used to take a backup of the swarm, UCP DTR and the DTR images in one go. 
You can pass this script an argument to override the default naming of the backup files. The following table 
shows the file names produced by <codeph>backup.sh</codeph> based on the argument passed in the command line.    
</p>

<table id="backup-utility-table-content" frame="none">
<title>Backup utility</title>
<tgroup cols="3">
<colspec colnum="1" colname="c1" align="left"/>
<colspec colnum="2" colname="c2" align="left"/>
<colspec colnum="3" colname="c3" align="left"/>
<thead>
<row>
<entry>Example</entry>
<entry>Command line</entry>
<entry>Generated filenames</entry>
</row>
</thead>
<tbody>
<row>
<entry>Default</entry>
<entry><codeph>./backup.sh</codeph></entry>
<entry>backup_swarm_&lt;vmname>_&lt;timestamp>.tgz, 
backup_ucp_&lt;vmname>_&lt;timestamp>.tgz, 
backup_dtr_meta_&lt;vmname>_&lt;timestamp>.tgz,
backup_dtr_data_&lt;vmname>_&lt;timestamp>.tgz
and the corresponding <codeph>.vars.tgz</codeph> files</entry>
</row>
<row>
<entry>Custom</entry>
<entry><codeph>./backup.sh my_backup</codeph></entry>
<entry>my_backup_swarm.tgz, my_backup_ucp.tgz, my_backup_dtr_meta.tgz, my_backup_dtr_data.tgz, and the corresponding <codeph>.vars.tgz</codeph> files</entry>
</row>
<row>
<entry>Date</entry>
<entry><codeph>./backup.sh $(date '+%Y_%m_%d_%H%M%S')</codeph></entry>
<entry>&lt;date>_swarm.tgz, &lt;date>_ucp.tgz, &lt;date>_dtr_meta.tgz,  &lt;date>_dtr_data.tgz, and the corresponding <codeph>.vars.tgz</codeph> files</entry>
</row>    
</tbody>
</tgroup>
</table>
    
</section>








</body>
<topic id="restore-ucp-dtr">
<title>Restoring your cluster after a disaster</title>
    
    
<body>
<section>
<title>Introduction</title>

<p>The playbooks address a disaster recovery scenario where you have lost your entire cluster and all the VMs. 
Other scenarios and how to handle them are described in the Docker documentation including the following scenarios:</p>
    
<ul>
<li>You have lost one UCP instance but your cluster still has the quorum. The easiest way is to recreate the missing UCP instance from scratch.</li> 
<li>You have lost the quorum in your UCP cluster but there is still one UCP instance running</li> 
<li>You have lost one instance of DTR but still have a quorum of replicas. The easiest way is to recreate the missing DTR instance from scratch.</li>
<li>You have lost the quorum of your DTR cluster but still have one DTR instance running.</li>    
</ul>    
  
</section> 

<section id="before-restore">
<title>Before you restore</title>    
 
<p><b>Step 1.</b> Retrieve the backup files using your chosen backup solution and save them to a 
folder on your ansible server. If you have used timestamps in the naming of your backup files, you can 
use them to determine the chronologial order. If you used the <codeph>backup.sh</codeph> script specifiying a date prefix,
you can use that to identify the matching set of backup files. You should 
choose the files in the following reverse chronological order, from the most recent to the oldest file. 
Make sure you restore both the *.tgz and the *.vars.tgz files.</p> 

<ol>
<li>DTR images backup</li>
<li>DTR metadata backup</li>
<li>UCP backup</li>
<li>Swarm backup</li>
</ol>
    
<p>In this example, we will assume a set of backup files stored in <codeph>/root/restore</codeph> that were created specifying a date prefix. These will have names like
<codeph>2018_04_17_151734_swarm.tgz</codeph>, <codeph>2018_04_17_151734_ucp.tgz</codeph>, etc and the corresponding 
<codeph>.vars.tgz</codeph> files.</p>

<p><b>Step 2:</b> Retrieve the DTR replica ID, the DTR version and the UCP version</p>
<p>To retrieve the ID of the replica that was backed up, as well as the version of DTR, you need to extract the data from the <codeph>.vars.tgz</codeph> file 
associated with the archive of the DTR metadata. You can retrieve this as follows:     
</p>
    
<codeblock>[root@clh2-ansible ops]# tar -Oxf /root/restore/2018_04_17_151734_dtr_meta.vars.tgz meta.yml
backup_node="clh-dtr01"
replica_id="<b>ad5204e8a4d0</b>"
backup_source=""
ucp_version=""
dtr_version="<b>2.4.3</b>"
</codeblock>

<codeblock>[root@clh2-ansible ops]# tar -Oxf /root/restore/2018_04_17_151734_ucp.vars.tgz meta.yml
backup_node="clh-ucp01"
replica_id=""
backup_source=""
ucp_version="<b>2.2.7</b>"
dtr_version=""</codeblock>

<p>Take note of the replica ID (ad5204e8a4d0), the version of DTR (2.4.3) and the version of UCP (2.2.7).</p>
    
<p><b>Step 3:</b> Populate the <codeph>group_vars/backups</codeph> file</p>   
 
 
<codeblock>backup_swarm: "/root/restore/2018_04_17_151734_swarm.tgz"
backup_ucp: "/root/restore/2018_04_17_151734_ucp.tgz"
backup_dtr_meta: "/root/restore/2018_04_17_151734_dtr_meta.tgz"
backup_dtr_data: "/root/restore/2018_04_17_151734_dtr_data.tgz"
backup_dtr_id: "ad5204e8a4d0"
backup_dest: "/root/backups"
backup_server: &lt;IP of your ansible box></codeblock> 
    
<p>You should populate your <codeph>group_vars/backups</codeph> file as above, with the <codeph>backup_dtr_id</codeph> variable
containing the value you retrieved in the preceding step as <codeph>replica_id="<b>ad5204e8a4d0</b>"</codeph></p>    
    
<p><b>Step 4:</b> Verify that your <codeph>group_vars/vars</codeph> file specifies the correct versions of DTR and UC</p>    

<p>The playbooks use the versions of UCP and DTR as specified in your <codeph>group_vars/vars</codeph> file to restore your backups.
You must ensure that the versions specified in your current <codeph>group_vars/vars</codeph> file correspond to the versions in the backups as determined above.
</p>
    
<codeblock>[root@clh2-ansible ops]# cat group_vars/vars | grep dtr_version
dtr_version: '2.4.3'
</codeblock>
<codeblock>
[root@clh2-ansible ops]# cat group_vars/vars | grep ucp_version
ucp_version: '2.2.7'
</codeblock>    

<p><b>Step 5:</b> Restore UCP admin credentials if required</p>
<p>You must ensure that the UCP admin credentials in your current <codeph>group_vars/vars</codeph> file are those
that were in effect when you generated the backup files. If they have changed since then, you must restore the original 
credentials for the duration of the restore procedure.</p>
 
<p><b>Step 6:</b> Restore your inventory (<codeph>vm_hosts</codeph>)</p> 
 
<p>Your inventory must reflect the enviroment that was present when the backup files were created. 
You can find a copy of the inventory as it was when the backup was taken in the  <codeph>*.vars.tgz</codeph> files. </p> 
</section>


<section>
<title>Restore UCP and DTR</title>  

<note type="warning">This procedure is aimed at restoring a cluster after a disaster. It assumes you have lost all 
the VMS in your cluster and want to redeploy using data that you backed up earlier.</note>

<p></p>
    
<ol>
<li>Ensure that you have completed all the preliminary steps as outlined in the section <xref href="backup-restore-ucp-dtr.dita#restore-ucp-dtr/before-restore" type="section">Before you restore</xref> </li>
<li>Run the restore playbook
<codeblock>ansible-playbook -i vm_hosts restore.yml</codeblock>
</li>
<li>If you are using the image scanning fuctionality in DTR, you will need to re-download the vilnerability database. 
For more information, see the Docker documentation  <xref href="https://docs.docker.com/datacenter/dtr/2.4/guides/admin/configure/set-up-vulnerability-scans/#get-the-security-scanning-license" format="html">here</xref>.</li>
</ol>
    
</section>
 
<section>
<title>Restore DTR metadata and DTR images</title>    

<note>This procedure restores DTR metadata and images and assumes you have lost all the DTR VMs in your cluster.  It will redeploy using the DTR data that you backed up earlier and will also restore the images if the folder exported by the NFS VM is empty.</note>


<ol>
<li>Ensure that you have completed all the preliminary steps as outlined in the section <xref href="backup-restore-ucp-dtr.dita#restore-ucp-dtr/before-restore" type="section">Before you restore</xref>. 
In this scenario, you need the archives for the DTR metadata and the DTR images.
</li>
<li>Ensure that all the DTR VMs listed in your inventory are destroyed, using the vSphere Web Client to delete them if required. If you want to restore the DTR images you should also delete the NFS VM.</li>
<li>Remove the DTR nodes from the swarm by running the <codeph>docker node rm &lt;DTR node></codeph> command on a UCP node for each DTR node in your cluster.
The following example shows the sequence of commands to use to remove the DTR nodes:
    
<codeblock>[root@clh2-ansible ops]# docker node ls
ID                            HOSTNAME                     STATUS              AVAILABILITY        MANAGER STATUS
aizd270nd7f0bsrxzu0vkdvfe *   clh-ucp02.cloudra.local      Ready               Active              Reachable
gvf30phdia7xvdyomnms2li8t     clh-dtr01.cloudra.local      Down                Active
ir4evcb4m84wtohuwmr6wuz11     clh-ucp03.cloudra.local      Ready               Active              Reachable
mwfsg9bxnk2z5q9cg05tbcs2f     clh-dtr02.cloudra.local      Down                Active
oqyulx95woy68w6ylcx111ug1     clh-ucp01.cloudra.local      Ready               Active              Leader
xqeq0sv3dhwgzbis0fx2x3h4a     clh-worker01.cloudra.local   Ready               Active
zdutvd4ajlh23zohd6xuf0p9e     clh-dtr03.cloudra.local      Down                Active
</codeblock>    


<codeblock>[root@clh2-ansible ops]# docker node rm clh-dtr01.cloudra.local
clh-dtr01.cloudra.local
[root@clh2-ansible ops]# docker node rm clh-dtr02.cloudra.local
clh-dtr02.cloudra.local
[root@clh2-ansible ops]# docker node rm clh-dtr03.cloudra.local
clh-dtr03.cloudra.local</codeblock>

<codeblock>[root@clh2-ansible ops]# docker node ls
ID                            HOSTNAME                     STATUS              AVAILABILITY        MANAGER STATUS
aizd270nd7f0bsrxzu0vkdvfe     clh-ucp02.cloudra.local      Ready               Active              Reachable
ir4evcb4m84wtohuwmr6wuz11     clh-ucp03.cloudra.local      Ready               Active              Reachable
oqyulx95woy68w6ylcx111ug1 *   clh-ucp01.cloudra.local      Ready               Active              Leader
xqeq0sv3dhwgzbis0fx2x3h4a     clh-worker01.cloudra.local   Ready               Active
</codeblock>
</li>

<li>Run the restore script:
<codeblock>./restore_dtr.sh</codeblock>        
</li>

<li>If you are using the image scanning fuctionality in DTR, you will need to re-download the vulnerability database. 
For more information, see the Docker documentation  <xref href="https://docs.docker.com/datacenter/dtr/2.4/guides/admin/configure/set-up-vulnerability-scans/#get-the-security-scanning-license" format="html">here</xref>.</li>

    
</ol>

</section> 
 
</body>   
</topic>    
    
    
</topic>
