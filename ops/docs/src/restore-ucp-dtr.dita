<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="restore-ucp-dtr">
<title>Restoring your cluster after a disaster</title>
    
    
<body>
<section>
<title>Introduction</title>

<p>The playbooks address a disaster recovery scenario where you have lost your entire cluster and all the VMs. 
Other scenarios and how to handle them are described in the Docker documentation including the following scenarios:</p>
    
<ul>
<li>You have lost one UCP instance but your cluster still has the quorum. The easiest way is to recreate the missing UCP instance from scratch.</li> 
<li>You have lost the quorum in your UCP cluster but there is still one UCP instance running</li> 
<li>You have lost one instance of DTR but still have a quorum of replicas. The easiest way is to recreate the missing DTR instance from scratch.</li>
<li>You have lost the quorum of your DTR cluster but still have one DTR instance running.</li>    
</ul>    
  

  
</section> 

<section id="before-restore">
<title>Before you restore</title>    
 
<p><b>Step 1.</b> Retrieve the backup files using your chosen backup solution and save them to a 
folder on your ansible server. If you have used timestamps in the naming of your backup files, you can 
use them to determine the chronologial order. If you used the <codeph>backup.sh</codeph> script specifiying a date prefix,
you can use that to identify the matching set of backup files. You should 
choose the files in the following reverse chronological order, from the most recent to the oldest file. 
Make sure you restore both the *.tgz and the *.vars.tgz files.</p> 

<ol>
<li>DTR images backup</li>
<li>DTR metadata backup</li>
<li>UCP backup</li>
<li>Swarm backup</li>
</ol>
    
<p>In this example, we will assume a set of backup files stored in <codeph>/root/restore</codeph> that were created specifying a date prefix. These will have names like
<codeph>2018_04_17_151734_swarm.tgz</codeph>, <codeph>2018_04_17_151734_ucp.tgz</codeph>, etc and the corresponding 
<codeph>.vars.tgz</codeph> files.</p>

<p><b>Step 2:</b> Retrieve the DTR replica ID, the DTR version and the UCP version</p>
<p>To retrieve the ID of the replica that was backed up, as well as the version of DTR, you need to extract the data from the <codeph>.vars.tgz</codeph> file 
associated with the archive of the DTR metadata. You can retrieve this as follows:     
</p>
    
<codeblock>[root@clh2-ansible ops]# tar -Oxf /root/restore/2018_04_17_151734_dtr_meta.vars.tgz meta.yml
backup_node="clh-dtr01"
replica_id="<b>ad5204e8a4d0</b>"
backup_source=""
ucp_version=""
dtr_version="<b>2.4.3</b>"
</codeblock>

<codeblock>[root@clh2-ansible ops]# tar -Oxf /root/restore/2018_04_17_151734_ucp.vars.tgz meta.yml
backup_node="clh-ucp01"
replica_id=""
backup_source=""
ucp_version="<b>2.2.7</b>"
dtr_version=""</codeblock>

<p>Take note of the replica ID (ad5204e8a4d0), the version of DTR (2.4.3) and the version of UCP (2.2.7).</p>
    
<p><b>Step 3:</b> Populate the <codeph>group_vars/backups</codeph> file</p>   
 
 
<codeblock>backup_swarm: "/root/restore/2018_04_17_151734_swarm.tgz"
backup_ucp: "/root/restore/2018_04_17_151734_ucp.tgz"
backup_dtr_meta: "/root/restore/2018_04_17_151734_dtr_meta.tgz"
backup_dtr_data: "/root/restore/2018_04_17_151734_dtr_data.tgz"
backup_dtr_id: "ad5204e8a4d0"
backup_dest: "/root/backups"
backup_server: &lt;IP of your ansible box></codeblock> 
    
<p>You should populate your <codeph>group_vars/backups</codeph> file as above, with the <codeph>backup_dtr_id</codeph> variable
containing the value you retrieved in the preceding step as <codeph>replica_id="<b>ad5204e8a4d0</b>"</codeph></p>    
    
<p><b>Step 4:</b> Verify that your <codeph>group_vars/vars</codeph> file specifies the correct versions of DTR and UC</p>    

<p>The playbooks use the versions of UCP and DTR as specified in your <codeph>group_vars/vars</codeph> file to restore your backups.
You must ensure that the versions specified in your current <codeph>group_vars/vars</codeph> file correspond to the versions in the backups as determined above.
</p>
    
<codeblock>[root@clh2-ansible ops]# cat group_vars/vars | grep dtr_version
dtr_version: '2.4.3'
</codeblock>
<codeblock>
[root@clh2-ansible ops]# cat group_vars/vars | grep ucp_version
ucp_version: '2.2.7'
</codeblock>    

<p><b>Step 5:</b> Restore UCP admin credentials if required</p>
<p>You must ensure that the UCP admin credentials in your current <codeph>group_vars/vars</codeph> file are those
that were in effect when you generated the backup files. If they have changed since then, you must restore the original 
credentials for the duration of the restore procedure.</p>
 
<p><b>Step 6:</b> Restore your inventory (<codeph>vm_hosts</codeph>)</p> 
 
<p>Your inventory must reflect the enviroment that was present when the backup files were created. 
You can find a copy of the inventory as it was when the backup was taken in the  <codeph>*.vars.tgz</codeph> files. </p> 
</section>


<section>
<title>Restore UCP and DTR</title>  

<note type="warning">This procedure is aimed at restoring a cluster after a disaster. It assumes you have lost all 
the VMs in your cluster and want to redeploy using data that you backed up earlier.

The solution follows Docker best practice, which means the swarm artifacts are not restored. 
You will need to restore your Docker volumes and you applications (stacks and services) when this procedure is complete.
</note>

<p></p>
    
<ol>
<li>Ensure that you have completed all the preliminary steps as outlined in the section <xref href="backup-restore-ucp-dtr.dita#restore-ucp-dtr/before-restore" type="section">Before you restore</xref> </li>
<li>Run the restore playbook
<codeblock>ansible-playbook -i vm_hosts restore.yml</codeblock>
</li>
<li>If you are using the image scanning fuctionality in DTR, you will need to re-download the vilnerability database. 
For more information, see the Docker documentation  <xref href="https://docs.docker.com/datacenter/dtr/2.4/guides/admin/configure/set-up-vulnerability-scans/#get-the-security-scanning-license" format="html">here</xref>.</li>
</ol>

<p>You are now ready to restore your Docker volumes and your applications.</p>
</section>
 
<section>
<title>Restore DTR metadata and DTR images</title>    

<note>This procedure restores DTR metadata and images and assumes you have lost all the DTR VMs in your cluster.  It will redeploy using the DTR data that you backed up earlier and will also restore the images if the folder exported by the NFS VM is empty.</note>


<ol>
<li>Ensure that you have completed all the preliminary steps as outlined in the section <xref href="backup-restore-ucp-dtr.dita#restore-ucp-dtr/before-restore" type="section">Before you restore</xref>. 
In this scenario, you need the archives for the DTR metadata and the DTR images.
</li>
<li>Ensure that all the DTR VMs listed in your inventory are destroyed, using the vSphere Web Client to delete them if required. If you want to restore the DTR images you should also delete the NFS VM.</li>
<li>Remove the DTR nodes from the swarm by running the <codeph>docker node rm &lt;DTR node></codeph> command on a UCP node for each DTR node in your cluster.
The following example shows the sequence of commands to use to remove the DTR nodes:
    
<codeblock>[root@clh2-ansible ops]# docker node ls
ID                            HOSTNAME                     STATUS              AVAILABILITY        MANAGER STATUS
aizd270nd7f0bsrxzu0vkdvfe *   clh-ucp02.cloudra.local      Ready               Active              Reachable
gvf30phdia7xvdyomnms2li8t     clh-dtr01.cloudra.local      Down                Active
ir4evcb4m84wtohuwmr6wuz11     clh-ucp03.cloudra.local      Ready               Active              Reachable
mwfsg9bxnk2z5q9cg05tbcs2f     clh-dtr02.cloudra.local      Down                Active
oqyulx95woy68w6ylcx111ug1     clh-ucp01.cloudra.local      Ready               Active              Leader
xqeq0sv3dhwgzbis0fx2x3h4a     clh-worker01.cloudra.local   Ready               Active
zdutvd4ajlh23zohd6xuf0p9e     clh-dtr03.cloudra.local      Down                Active
</codeblock>    


<codeblock>[root@clh2-ansible ops]# docker node rm clh-dtr01.cloudra.local
clh-dtr01.cloudra.local
[root@clh2-ansible ops]# docker node rm clh-dtr02.cloudra.local
clh-dtr02.cloudra.local
[root@clh2-ansible ops]# docker node rm clh-dtr03.cloudra.local
clh-dtr03.cloudra.local</codeblock>

<codeblock>[root@clh2-ansible ops]# docker node ls
ID                            HOSTNAME                     STATUS              AVAILABILITY        MANAGER STATUS
aizd270nd7f0bsrxzu0vkdvfe     clh-ucp02.cloudra.local      Ready               Active              Reachable
ir4evcb4m84wtohuwmr6wuz11     clh-ucp03.cloudra.local      Ready               Active              Reachable
oqyulx95woy68w6ylcx111ug1 *   clh-ucp01.cloudra.local      Ready               Active              Leader
xqeq0sv3dhwgzbis0fx2x3h4a     clh-worker01.cloudra.local   Ready               Active
</codeblock>
</li>

<li>Run the restore script:
<codeblock>./restore_dtr.sh</codeblock>        
</li>

<li>If you are using the image scanning fuctionality in DTR, you will need to re-download the vulnerability database. 
For more information, see the Docker documentation  <xref href="https://docs.docker.com/datacenter/dtr/2.4/guides/admin/configure/set-up-vulnerability-scans/#get-the-security-scanning-license" format="html">here</xref>.</li>

    
</ol>

</section> 
 
<section>
<title>Related playbooks</title>    

<ul>
<li><codeph>playbooks/backup_swarm.yml</codeph> is used to backup the swarm data</li>
<li><codeph>playbooks/backup_ucp.yml</codeph> is used to backup UCP</li>
<li><codeph>playbooks/backup_dtr_meta.yml</codeph> is used to backup DTR metadata</li>
<li><codeph>playbooks/backup_dtr_images.yml</codeph> is used to backup DTR images</li>
</ul>
    
</section> 
 
</body>   
</topic>
