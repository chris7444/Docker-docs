<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="playbooks-overview">
<title>Overview of the playbooks</title>
<body/>

<topic id="site">
<title>site.yml and related playbooks</title>
<body>  
 
 
<p>The playbook <codeph>./site.yml</codeph> is the day 0 playbook you use to deploy the solution. It is the main entry point and invokes the 
playbooks described in this section.</p>
    
<ul>
	
<li><codeph>playbooks/create_vms.yml</codeph> will create all the necessary Virtual Machines for 
the environment from the VM Template defined in the <codeph>vm_template</codeph> variable.</li>



<li><codeph>playbooks/config_networking.yml</codeph> will configure the network settings in all the Virtual Machines.</li>


<li><codeph>playbooks/config_subscription.yml</codeph> registers and subscribes all virtual machines to the Red Hat Customer Portal. 
This is only needed if you pull packages from Red Hat. This playbook is commented out by default but you should uncomment 
it to make sure each VM registers with the Red Hat portal. It is commented out so that you can test the deployment first 
without having to unregister all the VMs from the Red Hat Customer Portal between each test. If you are using an 
internal repository, as described in the section "Create a VM template", you can keep this playbook commented out.</li>



<li><codeph>playbooks/install_haproxy.yml</codeph> installs and configures the HAProxy package in the 
load balancer nodes. HAProxy is the tool chosen to implement load balancing for UCP nodes, DTR nodes and worker nodes.</li>


<li><codeph>playbooks/config_ntp.yml</codeph> configures the <b>chrony</b> client package in all Virtual Machines in 
order to have a synchronized clock across the environment. It will use the list of servers specified in the 
<codeph>ntp_servers</codeph> variable in the file <codeph>group_vars/vars</codeph>.</li>


<li><codeph>playbooks/install_docker.yml</codeph> installs Docker along with all its dependencies.</li>


<li><codeph>playbooks/install_rsyslog.yml</codeph> installs and configures <b>rsyslog</b> in the logger node 
and in all Docker nodes. The logger node will be configured to receive all <codeph>syslogs</codeph> on port 514 and 
the Docker nodes will be configured to send all logs (including container logs) to the logger node.</li>

<li><codeph>playbooks/config_docker_lvs.yml</codeph> performs a set of operations on the Docker nodes 
in order to create a partition on the second disk and carry out the LVM configuration, 
required for a sound Docker installation.</li>


<li><codeph>playbooks/docker_post_config.yml</codeph> performs a variety of tasks to complete 
the installation of the Docker environment, including configuration of the HTTP/HTTPS proxies, if any, 
and installation of the VMware vSphere Storage for Docker volume plugin.</li>


<li><codeph>playbooks/install_nfs_server.yml</codeph> installs and configures an NFS server on the NFS node.</li>


<li><codeph>playbooks/install_nfs_clients.yml</codeph> installs the required packages on the DTR nodes 
to be able to mount an NFS share.</li>


<li><codeph>playbooks/create_main_ucp.yml</codeph> installs and
configures the first Docker UCP instance on the target node
defined by the group <codeph>ucp_main</codeph> in the
<codeph>vm_hosts</codeph> inventory.</li>


<li><codeph>playbooks/scale_ucp.yml</codeph> installs and configures
additional instances of UCP on the target node defined by
the group <codeph>ucp</codeph> in the
vm_hosts inventory, except for the node defined in the group
<codeph>ucp_main</codeph>.</li>

	

<li><codeph>playbooks/create_main_dtr.yml</codeph> installs and
configures the first Docker DTR instance on the target nodes
defined by the group <codeph>dtr_main</codeph> in the <codeph>vm_hosts</codeph> inventory.</li>

	
<li><codeph>playbooks/scale_workers.yml</codeph> installs and configures additional workers 
on the target nodes defined by the group <codeph>worker</codeph> in the <codeph>vm_hosts</codeph> inventory.</li>

	

<li><codeph>playbooks/install_logspout.yml</codeph> installs and configures <b>Logspout</b> on all Docker nodes. 
Logspout is repsonsible for sending logs produced by containers running on the Docker nodes to the central logger VM.
By default, this playbook is commented out in <codeph>site.yml</codeph>.
</li>


<li><codeph>playbooks/config_monitoring.yml</codeph> configures a monitoring system for the Docker environment 
based on Grafana, Prometheus, cAdvisor and node-exporter Docker containers. By default, this playbook is commented out in <codeph>site.yml</codeph>, so if you want to use the solution to automatically
deploy a Prometheus/Grafana monitoring system, you must explicitly uncomment both this and the <codeph>playbooks/install_logspout.yml</codeph>
playbook.
</li>



<li><codeph>playbooks/monitoring.yml</codeph> installs and configures the Splunk Universal Forwarders.
The variable <codeph>monitoring_stack</codeph> in <codeph>group_vars/vars</codeph> is used to specify the type of deployment you want.	
A value of <codeph>splunk_demo</codeph> will result in this playbook deploying a splunk enterprise instance for demo purposes. A value of <codeph>splunk</codeph>
is used to configure an external production Splunk deployment.
</li>
    
<li><codeph>playbooks/splunk_demo.yml</codeph> installs a demo of Splunk Enterprise in the cluster (if the <codeph>splunk_demo</codeph> deployment option is selected)</li>    



<li><codeph>playbooks/config_scheduler.yml</codeph> configures the scheduler to prevent regular users 
(i.e. non-admin users) to schedule containers on the Docker nodes running instances of UCP and DTR.</li>
	

<li><codeph>playbooks/scale_dtr.yml</codeph> installs and configures additional instances (or replicas) 
of DTR on the target nodes defined by the group <codeph>dtr</codeph> in the <codeph>vm_hosts</codeph> inventory, 
with the exception of the node defined in the group <codeph>dtr_main</codeph>.</li>

	

<li><codeph>playbooks/reconfigure_dtr.yml</codeph>	is used to reconfigure DTR with the FQDN of the UCP Load Balancer and also 
enables image scanning.
</li>

<li><codeph>playbooks/install_sysdig.yml</codeph> is used to configure Sysdig. It opens the required port in the firewall, 
and installs the latest version of the Sysdig agent image on the nodes. By default, this playbook is commented out in <codeph>site.yml</codeph>, so if you want to use the solution to 
automatically configure Sysdig, you must uncomment this line.
</li>	
</ul>
    
</body> 
</topic>


<topic id="windows-playbooks">
<title>Windows playbooks</title>
    
<body>
    
<ul>  
<li><codeph>playbooks/create_windows_vms.yml</codeph> will create all the necessary Windows 2016 VMs 
for the environment based on the Windows VM Template defined in the <codeph>win_vm_template</codeph> variable.</li>   
    

<li><codeph>playbooks/install_docker_windows.yml</codeph> installs Docker along with all its dependencies 
on your Windows VMs</li>


<li><codeph>playbooks/scale_workers_win.yml</codeph> installs and configures additional Windows workers on the target nodes 
    defined by the group <codeph>win_worker</codeph> in the <codeph>vm_hosts</codeph> inventory.</li>
    
</ul> 
</body>    
</topic>
    

<topic id="backup-restore-playbooks">
<title>Backup and restore playbooks</title>
    
<body>
    
<section>
<title>Backup playbooks</title>
    
<ul>
<li><codeph>playbooks/backup_swarm.yml</codeph> is used to backup the swarm data</li>
<li><codeph>playbooks/backup_ucp.yml</codeph> is used to backup UCP</li>
<li><codeph>playbooks/backup_dtr_meta.yml</codeph> is used to backup DTR metadata</li>
<li><codeph>playbooks/backup_dtr_images.yml</codeph> is used to backup DTR images</li>
</ul>


</section>
    
<section>
<title>Restore playbooks</title>
    
<ul>
<li><codeph>playbooks/restore_dtr_images.yml</codeph> is used to restore DTR images</li>
<li><codeph>playbooks/restore_dtr_metadata.yml</codeph> is used to restore DTR metadata</li>
<li><codeph>playbooks/restore_ucp.yml</codeph> is used to restore UCP</li>
</ul>
    
    
</section>    
    

</body>
</topic>

<topic id="convenience-playbooks">
<title>Convenience playbooks</title>
    
<body>
 
<section>

<ul>
<li><codeph>playbooks/clean_all.yml</codeph> powers off and deletes all VMs in your inventory.</li>    
<li><codeph>playbooks/distribute_keys.yml</codeph> distributes public keys between all nodes, to allow each node to 
password-less log in to every other node. As this is not essential and can be regarded as a security risk 
(a worker node probably should not be able to log in to a UCP node, for instance), this playbook is not run from
 <codeph>site.yml</codeph> by default.</li>    
</ul>
    
</section> 
</body>        
</topic>

<topic id="convenience-scripts">
<title>Convenience scripts</title>
    
<body>
 
<section>

<ul>
<li><codeph>backup.sh</codeph> can be used to take a backup of the swarm, UCP DTR and the DTR images in one go.</li>
<li><codeph>restore_dtr.sh</codeph> can be used to restore DTR metadat and DTR images.</li>
<li><codeph>scale_worker.sh</codeph> can be used to scale the worker nodes.</li>
</ul>

 
</section>
</body>
</topic>
    
<!--	
        
monitoring_win.yml

internal:
collect_facts.yml
vsphere_plugin_win_ps.yml


???
config_monitoring_files.yml
config_simplivity_backups.yml


-->
</topic>
