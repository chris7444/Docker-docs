<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="solution-config">
<title>Solution configuration</title>
<body>
<p>By default, the Ansible playbooks are configured to set up a 3 node environment as this is the 
minimal starter configuration as recommended by HPE and Docker for 
production. The playbooks can also for larger container environments, with a 3 frame, 6 node HPE Synergy system, 
with 2 nodes in each frame. 
</p>

<p>Two separate configurations are available out of the box, with one restricted to a Linux-only deployment while the other
supports a hybrid deployment including Windows workers as well as Linux ones. 
The Docker and non-Docker modules are distributed over the physical nodes via
VMware virtual machines (VMs), depending on the size of your environment, as follows:</p>

<ul>
<li>3 Docker Universal Control Plane (UCP) VM nodes for HA and cluster management </li>
<li>3 Docker Trusted Registry (DTR) VM nodes for HA of the container registry </li>
</ul>

<p>The Docker UCP and DTR nodes are spread across 3 physical nodes, with one on each physical node. An odd number of manager nodes is
recommended to avoid split-brain issues. It is possible to restrict the deployment to 1 UCP and 1 DTR, or to expand to more than 3, but the recommended minimum for 
an enterprise production deployment is 3 UCPs and 3 DTRs.
</p>

<ul>  
<li>3 Docker Swarm Linux worker VM nodes for container workloads </li>
</ul>

<p>The Docker worker nodes will be co-located with the UCP and DTR nodes in a 3 physical node deployment, 
whereas in a 6 physical node set-up, the worker nodes will typically be separated onto the extra nodes. 
It is possible to specify that more than one worker node will be deployed per physical node but 
this decision will depend on the requirements of your applications.</p>

<ul>  
<li>1 Docker UCP load balancer VM to ensure access to UCP in the event of a node
failure </li>
<li>1 Docker DTR load balancer VM to ensure access to DTR in the event of a node
failure </li>
<li>1 Docker swarm worker node VM load balancer </li>
</ul>

<p>Three load balancers are provided to increase availability of the UCP, DTR and worker nodes and these are typically
distributed evenly across 3 physical nodes.</p>

<ul>  
<li>1 Logging server VM for central logging </li>
<li>1 NFS server VM for storage of Docker DTR images </li>
</ul>

<p>With the addition of the NFS and logging VMs, a total of 14 VMs are created for the default Linux-only deployment.  In addition to these VMs, the playbooks also set up the Docker persistent storage plug-in from VMware.  
The vSphere Docker volume plug-in  facilitates the storage of data in a shared datastore that can be accessed from any machine in the cluster.</p>


<ul>
<li>3 Docker Swarm Windows worker VM nodes for container workloads (optional) </li>     
</ul>
   
<p>The hybrid deployment will typically add 3 Windows worker nodes to this figure, co-located with the Linux workers. Note that some of the application 
    software supported by this configuration does not currently run on Windows, for example, the Sysdig Software Agent (see the 
    section <xref href="sysdig.dita"/>. </p>   

</body>
</topic>
